{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "847d13af",
   "metadata": {},
   "source": [
    "# Обработка датасета и упаковка HDF5\n",
    "\n",
    "Сбор кучи файлов в одний единый сжатый HDF5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4de0a9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07d1e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_path = \"../mt-grabber/archive_sorted\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d790441",
   "metadata": {},
   "outputs": [],
   "source": [
    "F_VER = \"1.0\"\n",
    "\n",
    "positions_dtype = np.dtype([\n",
    "    (\"ship_id\", \"i8\"),           # Внутренний идентификатор судна\n",
    "    (\"timestamp\", \"i4\"),         # Время отчета\n",
    "    (\"lat\", \"f4\"),               # Широта (WGS-84), в градусах\n",
    "    (\"lon\", \"f4\"),               # Долгота (WGS-84), в градусах\n",
    "    (\"speed\", \"i4\"),             # Скорость над грунтом (Speed over ground) в узлах\n",
    "    (\"course\", \"i4\"),            # Курс (Course over ground) (в градусах)\n",
    "    (\"heading\", \"i4\"),           # Направление носа судна (Heading), целое (в градусах)\n",
    "    (\"rot\", \"i4\"),               # Rate of Turn (изменение курса), в AIS шкале (-127..127)\n",
    "    (\"elapsed\", \"i4\"),           # Время с последнего отчета (секунды)\n",
    "    (\"destination\", \"S64\"),      # Указанный порт/пункт назначения (текст, неформализован)\n",
    "    (\"tile_z\", \"i4\"),            # Уровень зума тайла Marine Traffic\n",
    "    (\"file_id\", \"i4\"),           # Индекс файла в таблице `/files`, откуда поступила запись\n",
    "    (\"track_id\", \"i8\"),          # Идентификатор трека (маршрута)\n",
    "])\n",
    "\n",
    "ships_dtype = np.dtype([\n",
    "    (\"ship_id\", \"i8\"),           # Внутренний уникальный идентификатор\n",
    "    (\"mt_id\", \"S128\"),           # Marinetraffic ID\n",
    "    (\"name\", \"S128\"),            # Название судна\n",
    "    (\"flag\", \"S4\"),              # ISO-код страны флага (например, \"RU\", \"CN\")\n",
    "    (\"ship_type\", \"i4\"),         # AIS raw ship type\n",
    "    (\"gt_ship_type\", \"i4\"),      # Нормализованный/кластеризованный тип судна\n",
    "    (\"length\", \"i4\"),            # Длина судна (в метрах)\n",
    "    (\"width\", \"i4\"),             # Ширина судна (в метрах)\n",
    "    (\"dwt\", \"i4\"),               # Deadweight tonnage - дедвейт, тоннаж\n",
    "])\n",
    "\n",
    "tracks_dtype = np.dtype([\n",
    "    (\"track_id\", \"i8\"),          # Уникальный ID трека\n",
    "    (\"ship_id\", \"i8\"),           # Идентификатор судна\n",
    "    (\"start_timestamp\", \"i4\"),   # Время начала трека\n",
    "    (\"end_timestamp\", \"i4\"),     # Время окончания трека\n",
    "    (\"start_lat\", \"f4\"),         # Координаты начальной точки трека\n",
    "    (\"start_lon\", \"f4\"),\n",
    "    (\"end_lat\", \"f4\"),           # Координаты финальной точки трека\n",
    "    (\"end_lon\", \"f4\"),\n",
    "    (\"points_count\", \"i4\"),      # Количество точек в треке\n",
    "])\n",
    "\n",
    "files_dtype = np.dtype([\n",
    "    (\"file_id\", \"i4\"),           # Уникальный идентификатор файла\n",
    "    (\"name\", \"S256\"),            # Имя файла\n",
    "    (\"positions_count\", \"i4\"),   # Кол-во записей в файле\n",
    "    (\"timestamp\", \"i4\"),         # Время парсинга\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5327b7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подсчет размера папки + количества файлов\n",
    "def get_folder_stats(path):\n",
    "    total_size = 0\n",
    "    file_count = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(path):\n",
    "        for f in filenames:\n",
    "            try:\n",
    "                fp = os.path.join(dirpath, f)\n",
    "                if os.path.isfile(fp):\n",
    "                    total_size += os.path.getsize(fp)\n",
    "                    file_count += 1\n",
    "            except OSError:\n",
    "                # Игнорируем ошибки доступа к файлам\n",
    "                continue\n",
    "    # Перевод в гигабайты\n",
    "    size_gb = total_size / (1024 ** 3)\n",
    "    return size_gb, file_count\n",
    "\n",
    "source_stats = get_folder_stats(source_path)\n",
    "\n",
    "def create_hdf5(filename: str = \"mt_master.h5\"):\n",
    "    with h5py.File(filename, \"w\") as h5:\n",
    "        # Meta\n",
    "        h5.attrs[\"created_at\"] = datetime.utcnow().isoformat()\n",
    "        h5.attrs[\"version\"] = F_VER\n",
    "        h5.attrs[\"author\"] = \"Mark Vodyanitskiy (mvodya@icloud.com)\"\n",
    "        h5.attrs[\"sources_count\"] = source_stats[1]\n",
    "        h5.attrs[\"sources_size\"] = f\"{source_stats[0]:.4}Gb\"\n",
    "\n",
    "        # Empty datasets\n",
    "        h5.create_dataset(\"ships\", shape=(0,), maxshape=(None,), dtype=ships_dtype,\n",
    "                          chunks=True, compression=\"gzip\", compression_opts=4)\n",
    "\n",
    "        h5.create_dataset(\"files\", shape=(0,), maxshape=(None,), dtype=files_dtype,\n",
    "                          chunks=True, compression=\"gzip\", compression_opts=4)\n",
    "\n",
    "        h5.create_dataset(\"tracks\", shape=(0,), maxshape=(None,), dtype=tracks_dtype,\n",
    "                          chunks=True, compression=\"gzip\", compression_opts=4)\n",
    "\n",
    "    print(f\"Created HDF5 file: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a89697d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created HDF5 file: mt_master.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2c/2p1h64qd4zb1g13m_f5z4z9w0000gn/T/ipykernel_63034/2867499226.py:24: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  h5.attrs[\"created_at\"] = datetime.utcnow().isoformat()\n"
     ]
    }
   ],
   "source": [
    "create_hdf5()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2638c0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_json_files_by_date_range(dir: Path, start_date: str, end_date: str) -> list[Path]:\n",
    "    root_dir = Path(dir)\n",
    "    result = []\n",
    "    date_format = \"%d.%m.%Y\"\n",
    "    start = datetime.strptime(start_date, date_format)\n",
    "    end = datetime.strptime(end_date, date_format)\n",
    "\n",
    "    current = start\n",
    "    while current <= end:\n",
    "        year = f\"{current.year:04d}\"\n",
    "        month = f\"{current.month:02d}\"\n",
    "        day = f\"{current.day:02d}\"\n",
    "        dir_path = root_dir / year / month / day\n",
    "        if dir_path.exists():\n",
    "            result.extend(sorted(dir_path.glob(\"*.json\")))\n",
    "        current += timedelta(days=1)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c010dc48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/Volumes/SSD/mark/Documents/Works/MT_Dataset/archive/2024/10/29/29.10.2024_11_26.json'),\n",
       " PosixPath('/Volumes/SSD/mark/Documents/Works/MT_Dataset/archive/2024/10/29/29.10.2024_11_35.json')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_date = \"07.06.2021\"\n",
    "end_date = \"10.06.2026\"\n",
    "\n",
    "# Получаем список файлов\n",
    "files = get_json_files_by_date_range(source_path, start_date, end_date)\n",
    "files[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca376f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing JSON's:   0%|          | 49/27549 [00:06<1:01:19,  7.47it/s]/var/folders/2c/2p1h64qd4zb1g13m_f5z4z9w0000gn/T/ipykernel_63034/1754752334.py:28: DeprecationWarning: datetime.datetime.utcfromtimestamp() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.fromtimestamp(timestamp, datetime.UTC).\n",
      "  dt = datetime.utcfromtimestamp(ts)\n",
      "Parsing JSON's: 100%|██████████| 27549/27549 [3:07:42<00:00,  2.45it/s]   \n"
     ]
    }
   ],
   "source": [
    "# Маппинг локальных ID с идентификаторами marinetraffic\n",
    "mt_id_storage = {}\n",
    "next_ship_id = 1\n",
    "file_id = 0\n",
    "\n",
    "# Буферы\n",
    "ships_batch = []\n",
    "positions_batch = []\n",
    "files_batch = []\n",
    "\n",
    "def safe_int(value, default=-1):\n",
    "    try:\n",
    "        return int(value) if value is not None else default\n",
    "    except (ValueError, TypeError):\n",
    "        return default\n",
    "\n",
    "# Сохранение буферов\n",
    "def flush_to_hdf5(h5file, ships_batch, positions_batch, files_batch):\n",
    "    if ships_batch:\n",
    "        ds = h5file[\"ships\"]\n",
    "        old = ds.shape[0]\n",
    "        ds.resize((old + len(ships_batch),))\n",
    "        ds[old:] = np.array(ships_batch, dtype=ships_dtype)\n",
    "        ships_batch.clear()\n",
    "\n",
    "    if positions_batch:\n",
    "        grouped = {}\n",
    "        for row in positions_batch:\n",
    "            ts = int(row[1])  # timestamp (2-й столбец)\n",
    "            dt = datetime.utcfromtimestamp(ts)\n",
    "            key = (dt.year, dt.month, dt.day)\n",
    "            grouped.setdefault(key, []).append(row)\n",
    "\n",
    "        for (y, m, d), rows in grouped.items():\n",
    "            path = f\"positions/{y:04d}/{m:02d}\"\n",
    "            name = f\"{d:02d}\"\n",
    "            group = h5file.require_group(path)\n",
    "            if name not in group:\n",
    "                ds = group.create_dataset(\n",
    "                    name,\n",
    "                    shape=(0,),\n",
    "                    maxshape=(None,),\n",
    "                    dtype=positions_dtype,\n",
    "                    chunks=True,\n",
    "                    compression=\"gzip\",\n",
    "                    compression_opts=4,\n",
    "                )\n",
    "            else:\n",
    "                ds = group[name]\n",
    "            old = ds.shape[0]\n",
    "            ds.resize((old + len(rows),))\n",
    "            ds[old:] = np.array(rows, dtype=positions_dtype)\n",
    "\n",
    "        positions_batch.clear()\n",
    "\n",
    "    if files_batch:\n",
    "        ds = h5file[\"files\"]\n",
    "        old = ds.shape[0]\n",
    "        ds.resize((old + len(files_batch),))\n",
    "        ds[old:] = np.array(files_batch, dtype=files_dtype)\n",
    "        files_batch.clear()\n",
    "    \n",
    "    h5file.flush()\n",
    "\n",
    "flush_every = 50\n",
    "\n",
    "# Открываем и работаем с HDF файлом\n",
    "with h5py.File(\"mt_master.h5\", \"a\") as h5file:\n",
    "    # Перебираем файлы\n",
    "    for file in tqdm(files, desc=\"Parsing JSON's\"):\n",
    "        with open(file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            count = len(data)\n",
    "            files_batch.append((\n",
    "                file_id,\n",
    "                file.name,\n",
    "                count,\n",
    "                int(time.time())\n",
    "            ))\n",
    "            # Перебираем записи\n",
    "            for mt_id, record in data.items():\n",
    "                # Если судно еще не зарегистрированно - регистрируем\n",
    "                if mt_id not in mt_id_storage:\n",
    "                    mt_id_storage[mt_id] = next_ship_id\n",
    "                    next_ship_id += 1\n",
    "                    # Добавляем судно\n",
    "                    ships_batch.append((\n",
    "                        mt_id_storage[mt_id],\n",
    "                        mt_id,\n",
    "                        record.get(\"SHIPNAME\", \"null\"),\n",
    "                        record.get(\"FLAG\", \"null\"),\n",
    "                        safe_int(record.get(\"SHIPTYPE\", -1)),\n",
    "                        safe_int(record.get(\"GT_SHIPTYPE\", -1)),\n",
    "                        safe_int(record.get(\"LENGTH\", -1)),\n",
    "                        safe_int(record.get(\"WIDTH\", -1)),\n",
    "                        safe_int(record.get(\"DWT\", -1)),\n",
    "                    ))\n",
    "                ship_id = mt_id_storage[mt_id]\n",
    "\n",
    "                # Добываем данные о позиции\n",
    "                lat = float(record[\"LAT\"])\n",
    "                lon = float(record[\"LON\"])\n",
    "                tile_z = int(record[\"TILE_Z\"])\n",
    "\n",
    "                # Добавляем точку\n",
    "                positions_batch.append((\n",
    "                    ship_id,\n",
    "                    float(record[\"TIMESTAMP\"]),\n",
    "                    lat,\n",
    "                    lon,\n",
    "                    safe_int(record.get(\"SPEED\", -1)),\n",
    "                    safe_int(record.get(\"COURSE\", -1)),\n",
    "                    safe_int(record.get(\"HEADING\", -1)),\n",
    "                    safe_int(record.get(\"ROT\", 0)),\n",
    "                    safe_int(record.get(\"ELAPSED\", 0)),\n",
    "                    record.get(\"DESTINATION\", \"null\"),\n",
    "                    tile_z,\n",
    "                    file_id,\n",
    "                    -1, # track_id = unset\n",
    "                ))\n",
    "        file_id += 1\n",
    "\n",
    "        # Сброс буфферов в HDF\n",
    "        if file_id % flush_every == 0:\n",
    "            flush_to_hdf5(h5file, ships_batch, positions_batch, files_batch)\n",
    "    \n",
    "    # Финальный сброс буфферов в HDF\n",
    "    flush_to_hdf5(h5file, ships_batch, positions_batch, files_batch)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
