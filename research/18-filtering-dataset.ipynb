{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45ecc53d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/research/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cb89830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "author: Mark Vodyanitskiy (mvodya@icloud.com)\n",
      "created_at: 2025-07-13T14:26:08.378871\n",
      "sources_count: 27555\n",
      "sources_size: 439.3Gb\n",
      "version: 1.0\n"
     ]
    }
   ],
   "source": [
    "dataset_path = \"/Volumes/SSD/mark/Documents/Works/MT_Dataset/mt_master_20250714.h5\"\n",
    "\n",
    "if \"ds\" in vars():\n",
    "    ds.close()   # type: ignore\n",
    "\n",
    "ds = h5py.File(dataset_path, \"r\")\n",
    "for attr in ds.attrs:\n",
    "    print(f\"{attr}: {ds.attrs[attr]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6971d716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Chunk sizes ---\n",
    "CHUNK_ROWS_POSITIONS = 2_000_000\n",
    "CHUNK_ROWS_SHIPS = 5_000_000\n",
    "\n",
    "# --- Speed rules ---\n",
    "SPEED_MOVING_MIN = 10     # speed >= этого порога считаем \"движением\"\n",
    "SPEED_SANITY_MAX = 800    # все, что выше, считаем мусором и удаляем\n",
    "\n",
    "# --- Keep criteria ---\n",
    "MIN_TOTAL_POINTS  = 50\n",
    "MIN_MOVING_POINTS = 5\n",
    "MIN_MAX_SPEED     = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c010a218",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_day_datasets(ds: h5py.File):\n",
    "    \"\"\"\n",
    "    Yield tuples: (('YYYY','MM','DD'), dataset)\n",
    "    \"\"\"\n",
    "    if \"positions\" not in ds:\n",
    "        return\n",
    "    gpos = ds[\"positions\"]\n",
    "    for yyyy in sorted(gpos.keys()):\n",
    "        gy = gpos[yyyy]\n",
    "        if not isinstance(gy, h5py.Group):\n",
    "            continue\n",
    "        for mm in sorted(gy.keys()):\n",
    "            gm = gy[mm]\n",
    "            if not isinstance(gm, h5py.Group):\n",
    "                continue\n",
    "            for dd in sorted(gm.keys()):\n",
    "                dsd = gm[dd]\n",
    "                if isinstance(dsd, h5py.Dataset):\n",
    "                    yield (yyyy, mm, dd), dsd\n",
    "\n",
    "\n",
    "def ensure_group(h5: h5py.File, path: str) -> h5py.Group:\n",
    "    \"\"\"\n",
    "    Create nested groups like mkdir -p. Returns final group.\n",
    "    path like: \"positions/2024/10\"\n",
    "    \"\"\"\n",
    "    g = h5\n",
    "    for part in [p for p in path.split(\"/\") if p]:\n",
    "        if part not in g:\n",
    "            g = g.create_group(part)\n",
    "        else:\n",
    "            g = g[part]\n",
    "    return g\n",
    "\n",
    "\n",
    "def append_rows(dst_ds: h5py.Dataset, rows: np.ndarray) -> None:\n",
    "    \"\"\"\n",
    "    Append structured rows to resizable 1D dataset.\n",
    "    \"\"\"\n",
    "    if rows.size == 0:\n",
    "        return\n",
    "    old = dst_ds.shape[0]\n",
    "    new = old + rows.shape[0]\n",
    "    dst_ds.resize((new,))\n",
    "    dst_ds[old:new] = rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d80f9f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Days: 247\n",
      "Total positions: 1250874033\n"
     ]
    }
   ],
   "source": [
    "days = []\n",
    "total_positions = 0\n",
    "\n",
    "for (yyyy, mm, dd), day_ds in iter_day_datasets(ds):\n",
    "    days.append(((yyyy, mm, dd), day_ds))\n",
    "    total_positions += int(day_ds.shape[0])\n",
    "\n",
    "print(\"Days:\", len(days))\n",
    "print(\"Total positions:\", total_positions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9cd05245",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pass0: scan ships for max_ship_id: 100%|██████████| 156832602/156832602 [01:28<00:00, 1764832.67rows/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_ship_id = 156832602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ships_src = ds[\"ships\"]\n",
    "n_ships = int(ships_src.shape[0])\n",
    "\n",
    "max_ship_id = 0\n",
    "p0 = tqdm(total=n_ships, desc=\"Pass0: scan ships for max_ship_id\", unit=\"rows\")\n",
    "\n",
    "for start in range(0, n_ships, CHUNK_ROWS_SHIPS):\n",
    "    end = min(n_ships, start + CHUNK_ROWS_SHIPS)\n",
    "    chunk = ships_src[start:end]\n",
    "    if chunk.size:\n",
    "        m = int(chunk[\"ship_id\"].max(initial=0))\n",
    "        if m > max_ship_id:\n",
    "            max_ship_id = m\n",
    "    p0.update(end - start)\n",
    "\n",
    "p0.close()\n",
    "print(\"max_ship_id =\", max_ship_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b11ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pass1: scan positions (stats): 100%|██████████| 1250874033/1250874033 [09:49<00:00, 2120348.72rows/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stats computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "total_points  = np.zeros(max_ship_id + 1, dtype=np.uint32)\n",
    "moving_points = np.zeros(max_ship_id + 1, dtype=np.uint32)\n",
    "max_speed     = np.zeros(max_ship_id + 1, dtype=np.uint16)\n",
    "\n",
    "p1 = tqdm(total=total_positions, desc=\"Pass1: scan positions (stats)\", unit=\"rows\")\n",
    "\n",
    "for (yyyy, mm, dd), day_ds in days:\n",
    "    n = int(day_ds.shape[0])\n",
    "\n",
    "    for start in range(0, n, CHUNK_ROWS_POSITIONS):\n",
    "        end = min(n, start + CHUNK_ROWS_POSITIONS)\n",
    "        chunk = day_ds[start:end]\n",
    "\n",
    "        ship_ids = chunk[\"ship_id\"].astype(np.int64, copy=False)\n",
    "\n",
    "        sp = chunk[\"speed\"].astype(np.int32, copy=False)\n",
    "        sp_sane = np.clip(sp, 0, SPEED_SANITY_MAX).astype(np.int32, copy=False)\n",
    "        moving_mask = sp_sane >= SPEED_MOVING_MIN\n",
    "\n",
    "        # Total points per ship in this chunk\n",
    "        u, c = np.unique(ship_ids, return_counts=True)\n",
    "        total_points[u] += c.astype(np.uint32, copy=False)\n",
    "\n",
    "        # Moving points per ship\n",
    "        if moving_mask.any():\n",
    "            mv_ids = ship_ids[moving_mask]\n",
    "            u2, c2 = np.unique(mv_ids, return_counts=True)\n",
    "            moving_points[u2] += c2.astype(np.uint32, copy=False)\n",
    "\n",
    "        # Max speed per ship\n",
    "        np.maximum.at(max_speed, ship_ids, sp_sane.astype(np.uint16, copy=False))\n",
    "\n",
    "        p1.update(end - start)\n",
    "\n",
    "p1.close()\n",
    "print(\"Stats computed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a5d1c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kept ships: 372,403 / (mask size=156,832,603)\n"
     ]
    }
   ],
   "source": [
    "keep_mask = (\n",
    "    (total_points >= MIN_TOTAL_POINTS) &\n",
    "    (moving_points >= MIN_MOVING_POINTS) &\n",
    "    (max_speed >= MIN_MAX_SPEED)\n",
    ")\n",
    "\n",
    "kept_ship_count = int(keep_mask.sum())\n",
    "print(f\"Kept ships: {kept_ship_count:,} / (mask size={keep_mask.size:,})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277996b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Destination file created: /Volumes/SSD/mark/Documents/Works/MT_Dataset/mt_filtered_20250714.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2c/2p1h64qd4zb1g13m_f5z4z9w0000gn/T/ipykernel_19364/2301459398.py:13: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  dst.attrs[\"filtered_at\"] = datetime.utcnow().isoformat()\n"
     ]
    }
   ],
   "source": [
    "out_path = \"/Volumes/SSD/mark/Documents/Works/MT_Dataset/mt_filtered_20250714.h5\"\n",
    "\n",
    "dst = h5py.File(out_path, \"w\")\n",
    "\n",
    "# Root attrs\n",
    "for k, v in ds.attrs.items():\n",
    "    dst.attrs[k] = v\n",
    "\n",
    "dst.attrs[\"filtered_at\"] = datetime.utcnow().isoformat()\n",
    "dst.attrs[\"filter_rules\"] = (\n",
    "    f\"MIN_TOTAL_POINTS={MIN_TOTAL_POINTS}, \"\n",
    "    f\"MIN_MOVING_POINTS={MIN_MOVING_POINTS}, \"\n",
    "    f\"MIN_MAX_SPEED={MIN_MAX_SPEED}, \"\n",
    "    f\"SPEED_MOVING_MIN={SPEED_MOVING_MIN}, \"\n",
    "    f\"SPEED_SANITY_MAX={SPEED_SANITY_MAX}\"\n",
    ")\n",
    "\n",
    "# Copy static datasets\n",
    "if \"files\" in ds:\n",
    "    ds.copy(\"files\", dst)\n",
    "if \"zones\" in ds:\n",
    "    ds.copy(\"zones\", dst)\n",
    "\n",
    "# tracks: пустой, dtype берем из исходника если есть\n",
    "tracks_dtype = ds[\"tracks\"].dtype if \"tracks\" in ds else np.dtype([(\"track_id\", \"i8\")])\n",
    "dst.create_dataset(\n",
    "    \"tracks\",\n",
    "    shape=(0,), maxshape=(None,),\n",
    "    dtype=tracks_dtype,\n",
    "    chunks=True, compression=\"gzip\", compression_opts=4\n",
    ")\n",
    "\n",
    "# Ships output\n",
    "ships_dtype = ships_src.dtype\n",
    "ships_dst = dst.create_dataset(\n",
    "    \"ships\",\n",
    "    shape=(0,), maxshape=(None,),\n",
    "    dtype=ships_dtype,\n",
    "    chunks=True, compression=\"gzip\", compression_opts=4\n",
    ")\n",
    "\n",
    "# Positions root group\n",
    "ensure_group(dst, \"positions\")\n",
    "\n",
    "print(\"Destination file created:\", out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47a0ff89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pass2: write ships: 100%|██████████| 156832602/156832602 [03:19<00:00, 787320.78rows/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ships written: 372403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "p2 = tqdm(total=n_ships, desc=\"Pass2: write ships\", unit=\"rows\")\n",
    "\n",
    "for start in range(0, n_ships, CHUNK_ROWS_SHIPS):\n",
    "    end = min(n_ships, start + CHUNK_ROWS_SHIPS)\n",
    "    chunk = ships_src[start:end]\n",
    "    ids = chunk[\"ship_id\"].astype(np.int64, copy=False)\n",
    "    m = keep_mask[ids]\n",
    "    kept = chunk[m]\n",
    "    append_rows(ships_dst, kept)\n",
    "    p2.update(end - start)\n",
    "\n",
    "p2.close()\n",
    "print(\"ships written:\", ships_dst.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcec0027",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pass3: write positions: 100%|██████████| 1250874033/1250874033 [27:55<00:00, 746346.28rows/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positions written.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "p3 = tqdm(total=total_positions, desc=\"Pass3: write positions\", unit=\"rows\")\n",
    "\n",
    "for (yyyy, mm, dd), day_src in days:\n",
    "    # positions/YYYY/MM\n",
    "    g = ensure_group(dst, f\"positions/{yyyy}/{mm}\")\n",
    "\n",
    "    # Create day dataset resizable\n",
    "    day_dst = g.create_dataset(\n",
    "        dd,\n",
    "        shape=(0,), maxshape=(None,),\n",
    "        dtype=day_src.dtype,\n",
    "        chunks=True, compression=\"gzip\", compression_opts=4\n",
    "    )\n",
    "\n",
    "    n = int(day_src.shape[0])\n",
    "    for start in range(0, n, CHUNK_ROWS_POSITIONS):\n",
    "        end = min(n, start + CHUNK_ROWS_POSITIONS)\n",
    "        chunk = day_src[start:end]\n",
    "        ids = chunk[\"ship_id\"].astype(np.int64, copy=False)\n",
    "        m = keep_mask[ids]\n",
    "        kept = chunk[m]\n",
    "        append_rows(day_dst, kept)\n",
    "        p3.update(end - start)\n",
    "\n",
    "p3.close()\n",
    "print(\"Positions written\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7672e5ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done: /Volumes/SSD/mark/Documents/Works/MT_Dataset/mt_filtered_20250714.h5\n"
     ]
    }
   ],
   "source": [
    "dst.close()\n",
    "print(out_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
