{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f415a838",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0bf3d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "author: Mark Vodyanitskiy (mvodya@icloud.com)\n",
      "created_at: 2025-07-13T14:26:08.378871\n",
      "filter_rules: MIN_TOTAL_POINTS=50, MIN_MOVING_POINTS=5, MIN_MAX_SPEED=20, SPEED_MOVING_MIN=10, SPEED_SANITY_MAX=800\n",
      "filtered_at: 2026-01-08T06:02:47.931391\n",
      "sources_count: 27555\n",
      "sources_size: 439.3Gb\n",
      "tracks_built_at: 2026-01-09T17:19:14.225437\n",
      "tracks_count: 11731643\n",
      "tracks_rules: SPEED_MOVING_MIN=10, STOP_RADIUS_M=250.0, STOP_DWELL_SEC=1800, GAP_HARD_SEC=18000, GAP_VERY_HARD_SEC=36000, DIST_AFTER_GAP_M=150000.0, JUMP_HARD_M=250000.0, DEST_GAP_SEC=14400, DEST_DIST_M=50000.0\n",
      "version: 1.0\n"
     ]
    }
   ],
   "source": [
    "dataset_path = \"/Volumes/SSD/mark/Documents/Works/MT_Dataset/mt_tracks_20250714.h5\"\n",
    "\n",
    "if \"ds\" in vars():\n",
    "    ds.close()   # type: ignore\n",
    "\n",
    "ds = h5py.File(dataset_path, \"r\")\n",
    "for attr in ds.attrs:\n",
    "    print(f\"{attr}: {ds.attrs[attr]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6627b7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_PATH = \"/Volumes/SSD/mark/Documents/Works/MT_Dataset/mt_tracks_20250714_tsorted.h5\"\n",
    "POI_JSON_PATH = \"/Volumes/SSD/mark/Documents/Works/MT_Dataset/mt_tracks_20250714_poi.json\"\n",
    "\n",
    "# Фильтрация по POI\n",
    "USE_POI_FILTER = True\n",
    "\n",
    "# Layout: /positions/tracks/AAAA-BBBB/CCCC-DDDD\n",
    "TRACKS_PER_GROUP = 100_000      # ширина диапазона track_id для группы AAAA-BBBB\n",
    "DATASETS_PER_GROUP = 100        # внутри группы нарезаем на 100 датасетов CCCC-DDDD\n",
    "\n",
    "# IO/memory\n",
    "READ_CHUNK_ROWS = 2_000_000       # сколько строк читать за раз из /positions/YYYY/MM/DD\n",
    "FLUSH_THRESHOLD_ROWS = 2_000_000  # сколько строк накапливаем на (group,sub) перед flush\n",
    "\n",
    "# Копировать ли исходный /positions в новый файл\n",
    "COPY_ORIGINAL_POSITIONS = True\n",
    "\n",
    "# Какие поля пишем в tsorted-структуру\n",
    "TSORT_FIELDS = [\n",
    "    \"track_id\", \"timestamp\", \"lat\", \"lon\",\n",
    "    \"speed\", \"course\", \"heading\", \"rot\", \"elapsed\",\n",
    "    \"ship_id\", \"file_id\", \"destination\", \"tile_z\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d73dbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_parent_dir(path: str):\n",
    "    Path(path).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def range_label(a: int, b: int) -> str:\n",
    "    # 0-padded to keep lexicographic ordering nice\n",
    "    return f\"{a:08d}-{b:08d}\"\n",
    "\n",
    "def compute_group_bounds(track_id: int, tracks_per_group: int):\n",
    "    g0 = (track_id // tracks_per_group) * tracks_per_group\n",
    "    g1 = g0 + tracks_per_group - 1\n",
    "    return g0, g1\n",
    "\n",
    "def compute_subrange_bounds(track_id: int, g0: int, subrange: int):\n",
    "    s_idx = (track_id - g0) // subrange\n",
    "    s0 = g0 + s_idx * subrange\n",
    "    s1 = min(s0 + subrange - 1, g0 + TRACKS_PER_GROUP - 1)\n",
    "    return int(s_idx), s0, s1\n",
    "\n",
    "def stable_sort_by_track_then_time(arr: np.ndarray) -> np.ndarray:\n",
    "    # stable: first sort by timestamp, then stable sort by track_id\n",
    "    # mergesort is stable\n",
    "    idx_time = np.argsort(arr[\"timestamp\"], kind=\"mergesort\")\n",
    "    arr2 = arr[idx_time]\n",
    "    idx_track = np.argsort(arr2[\"track_id\"], kind=\"mergesort\")\n",
    "    return arr2[idx_track]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a290deb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading POI json: /Volumes/SSD/mark/Documents/Works/MT_Dataset/mt_tracks_20250714_poi.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collect POI track_ids: 100%|██████████| 6003/6003 [00:00<00:00, 336735.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POI tracks: 380072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "poi_track_ids = None\n",
    "\n",
    "if USE_POI_FILTER:\n",
    "    print(\"Loading POI json:\", POI_JSON_PATH)\n",
    "    with open(POI_JSON_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        poi = json.load(f)\n",
    "\n",
    "    # pairs: \"A->B\": { ..., \"track_ids\": [...] }\n",
    "    # собираем все track_ids в set\n",
    "    poi_track_ids = set()\n",
    "    pairs = poi.get(\"pairs\", {})\n",
    "    for k, v in tqdm(pairs.items(), desc=\"Collect POI track_ids\"):\n",
    "        tids = v.get(\"track_ids\", [])\n",
    "        poi_track_ids.update(tids)\n",
    "\n",
    "    # safety: track_ids могут быть int/str - нормализуем в int\n",
    "    poi_track_ids = {int(x) for x in poi_track_ids}\n",
    "\n",
    "    print(\"POI tracks:\", len(poi_track_ids))\n",
    "else:\n",
    "    print(\"POI filter disabled.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "765087cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copy: files\n",
      "Copy: ships\n",
      "Copy: tracks\n",
      "Copy: zones\n",
      "Copy: positions (this can double file size!)\n",
      "Output ready: /Volumes/SSD/mark/Documents/Works/MT_Dataset/mt_tracks_20250714_tsorted.h5\n"
     ]
    }
   ],
   "source": [
    "ensure_parent_dir(OUT_PATH)\n",
    "\n",
    "# Подготовим dtype для tsorted-позиций как subset исходного dtype\n",
    "# Найдем любой positions dataset чтобы взять dtype (первый попавшийся день)\n",
    "def find_any_positions_dataset(ds) -> h5py.Dataset:\n",
    "    pos_root = ds[\"positions\"]\n",
    "    for y in pos_root:\n",
    "        for m in pos_root[y]:\n",
    "            for d in pos_root[y][m]:\n",
    "                return pos_root[y][m][d]\n",
    "    raise RuntimeError(\"No positions datasets found in ds['positions'].\")\n",
    "\n",
    "src_any_pos = find_any_positions_dataset(ds)\n",
    "src_dtype = src_any_pos.dtype\n",
    "ts_dtype = np.dtype([(name, src_dtype.fields[name][0]) for name in TSORT_FIELDS])\n",
    "\n",
    "# Открываем output\n",
    "out = h5py.File(OUT_PATH, \"w\")\n",
    "\n",
    "# attrs\n",
    "for k, v in ds.attrs.items():\n",
    "    out.attrs[k] = v\n",
    "out.attrs[\"tsorted_built_at\"] = np.string_(str(np.datetime64(\"now\")))\n",
    "\n",
    "# копируем верхнеуровневые таблицы (без positions)\n",
    "for name in [\"files\", \"ships\", \"tracks\", \"zones\"]:\n",
    "    if name in ds:\n",
    "        print(\"Copy:\", name)\n",
    "        ds.copy(name, out, name=name)\n",
    "\n",
    "# positions: копируем опционально\n",
    "if COPY_ORIGINAL_POSITIONS:\n",
    "    print(\"Copy: positions (this can double file size!)\")\n",
    "    ds.copy(\"positions\", out, name=\"positions\")\n",
    "else:\n",
    "    # создаем пустой positions root (чтобы структура была совместима)\n",
    "    out.create_group(\"positions\")\n",
    "\n",
    "# создаем /positions/tracks для новой структуры\n",
    "tracks_root = out[\"positions\"].require_group(\"tracks\")\n",
    "\n",
    "print(\"Output ready:\", OUT_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b171d64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_days: 247\n"
     ]
    }
   ],
   "source": [
    "TQDM_KW = dict(\n",
    "    ascii=True,          # гарантированный \"текстовый\" прогрессбар\n",
    "    dynamic_ncols=True,  # подстраивается под ширину\n",
    "    mininterval=0.5,\n",
    ")\n",
    "\n",
    "pos_root = ds[\"positions\"]\n",
    "\n",
    "# считаем количество дней заранее (total для прогресса)\n",
    "years = sorted(pos_root.keys())\n",
    "\n",
    "total_days = 0\n",
    "for y in years:\n",
    "    for m in pos_root[y].keys():\n",
    "        total_days += len(pos_root[y][m].keys())\n",
    "\n",
    "print(\"total_days:\", total_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d713278b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repack days: 100%|##########| 247/247 [05:11<00:00,  1.26s/it, 2025/07/10 rows=1401756]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final flush...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Flush leftovers: 100%|##########| 13781/13781 [00:11<00:00, 1200.65it/s]\n"
     ]
    }
   ],
   "source": [
    "SUBRANGE = math.ceil(TRACKS_PER_GROUP / DATASETS_PER_GROUP)\n",
    "\n",
    "ds_cache = {}\n",
    "\n",
    "def get_or_create_tsorted_dataset(out_tracks_root: h5py.Group, g0: int, g1: int, s0: int, s1: int) -> h5py.Dataset:\n",
    "    g_label = range_label(g0, g1)\n",
    "    s_label = range_label(s0, s1)\n",
    "\n",
    "    g = out_tracks_root.require_group(g_label)\n",
    "    path_key = f\"{g_label}/{s_label}\"\n",
    "\n",
    "    dset = ds_cache.get(path_key)\n",
    "    if dset is not None:\n",
    "        return dset\n",
    "\n",
    "    if s_label in g:\n",
    "        dset = g[s_label]\n",
    "    else:\n",
    "        dset = g.create_dataset(\n",
    "            s_label,\n",
    "            shape=(0,),\n",
    "            maxshape=(None,),\n",
    "            dtype=ts_dtype,\n",
    "            chunks=True,\n",
    "            compression=\"gzip\",\n",
    "            compression_opts=4\n",
    "        )\n",
    "\n",
    "    ds_cache[path_key] = dset\n",
    "    return dset\n",
    "\n",
    "def append_rows(dset: h5py.Dataset, rows: np.ndarray):\n",
    "    n0 = dset.shape[0]\n",
    "    n_add = rows.shape[0]\n",
    "    dset.resize((n0 + n_add,))\n",
    "    dset[n0:n0 + n_add] = rows\n",
    "\n",
    "buffers = defaultdict(list)\n",
    "buffer_sizes = defaultdict(int)\n",
    "\n",
    "# главный прогрессбар по дням\n",
    "p_days = tqdm(total=total_days, desc=\"Repack days\", **TQDM_KW)\n",
    "\n",
    "for y in years:\n",
    "    months = sorted(pos_root[y].keys())\n",
    "    for m in months:\n",
    "        days = sorted(pos_root[y][m].keys())\n",
    "        for d in days:\n",
    "            day_ds = pos_root[y][m][d]\n",
    "            n = day_ds.shape[0]\n",
    "\n",
    "            # обновляем postfix чтобы было видно где мы\n",
    "            p_days.set_postfix_str(f\"{y}/{m}/{d} rows={n}\")\n",
    "\n",
    "            if n == 0:\n",
    "                p_days.update(1)\n",
    "                continue\n",
    "\n",
    "            for start in range(0, n, READ_CHUNK_ROWS):\n",
    "                end = min(start + READ_CHUNK_ROWS, n)\n",
    "                chunk = day_ds[start:end]\n",
    "\n",
    "                # subset полей -> ts_dtype\n",
    "                chunk2 = np.empty((chunk.shape[0],), dtype=ts_dtype)\n",
    "                for name in TSORT_FIELDS:\n",
    "                    chunk2[name] = chunk[name]\n",
    "\n",
    "                # POI фильтр\n",
    "                if USE_POI_FILTER:\n",
    "                    tids = chunk2[\"track_id\"].astype(np.int64, copy=False)\n",
    "                    # быстрее чем генератор по одному элементу:\n",
    "                    # np.isin на больших массивах может быть тяжелым по памяти,\n",
    "                    # поэтому делаем через vectorized-проверку на python set:\n",
    "                    mask = np.fromiter((int(t) in poi_track_ids for t in tids),\n",
    "                                       dtype=np.bool_, count=tids.shape[0])\n",
    "                    if not mask.any():\n",
    "                        continue\n",
    "                    chunk2 = chunk2[mask]\n",
    "                    if chunk2.shape[0] == 0:\n",
    "                        continue\n",
    "\n",
    "                # распределяем по (group, subrange) без построчного цикла\n",
    "                tids = chunk2[\"track_id\"].astype(np.int64, copy=False)\n",
    "                g0s = (tids // TRACKS_PER_GROUP) * TRACKS_PER_GROUP\n",
    "                s_idxs = ((tids - g0s) // SUBRANGE).astype(np.int64, copy=False)\n",
    "\n",
    "                order = np.lexsort((s_idxs, g0s))\n",
    "                chunk2 = chunk2[order]\n",
    "                g0s = g0s[order]\n",
    "                s_idxs = s_idxs[order]\n",
    "\n",
    "                change = np.empty(chunk2.shape[0], dtype=np.bool_)\n",
    "                change[0] = True\n",
    "                change[1:] = (g0s[1:] != g0s[:-1]) | (s_idxs[1:] != s_idxs[:-1])\n",
    "\n",
    "                cuts = np.nonzero(change)[0]\n",
    "                cuts = np.append(cuts, chunk2.shape[0])\n",
    "\n",
    "                for i in range(len(cuts) - 1):\n",
    "                    a, b = cuts[i], cuts[i + 1]\n",
    "                    g0 = int(g0s[a])\n",
    "                    g1 = g0 + TRACKS_PER_GROUP - 1\n",
    "                    s_idx = int(s_idxs[a])\n",
    "                    s0 = g0 + s_idx * SUBRANGE\n",
    "                    s1 = min(s0 + SUBRANGE - 1, g1)\n",
    "\n",
    "                    part = chunk2[a:b]\n",
    "                    key = (g0, g1, s0, s1)\n",
    "                    buffers[key].append(part)\n",
    "                    buffer_sizes[key] += part.shape[0]\n",
    "\n",
    "                    if buffer_sizes[key] >= FLUSH_THRESHOLD_ROWS:\n",
    "                        merged = np.concatenate(buffers[key], axis=0)\n",
    "                        merged = stable_sort_by_track_then_time(merged)\n",
    "\n",
    "                        dset = get_or_create_tsorted_dataset(tracks_root, g0, g1, s0, s1)\n",
    "                        append_rows(dset, merged)\n",
    "\n",
    "                        buffers[key].clear()\n",
    "                        buffer_sizes[key] = 0\n",
    "\n",
    "            p_days.update(1)\n",
    "\n",
    "p_days.close()\n",
    "\n",
    "# финальный flush\n",
    "print(\"Final flush...\")\n",
    "for key, parts in tqdm(list(buffers.items()), desc=\"Flush leftovers\", **TQDM_KW):\n",
    "    if not parts:\n",
    "        continue\n",
    "    g0, g1, s0, s1 = key\n",
    "    merged = np.concatenate(parts, axis=0)\n",
    "    merged = stable_sort_by_track_then_time(merged)\n",
    "    dset = get_or_create_tsorted_dataset(tracks_root, g0, g1, s0, s1)\n",
    "    append_rows(dset, merged)\n",
    "\n",
    "out.flush()\n",
    "out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45c434a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "author: Mark Vodyanitskiy (mvodya@icloud.com)\n",
      "created_at: 2025-07-13T14:26:08.378871\n",
      "filter_rules: MIN_TOTAL_POINTS=50, MIN_MOVING_POINTS=5, MIN_MAX_SPEED=20, SPEED_MOVING_MIN=10, SPEED_SANITY_MAX=800\n",
      "filtered_at: 2026-01-08T06:02:47.931391\n",
      "sources_count: 27555\n",
      "sources_size: 439.3Gb\n",
      "tracks_built_at: 2026-01-09T17:19:14.225437\n",
      "tracks_count: 11731643\n",
      "tracks_rules: SPEED_MOVING_MIN=10, STOP_RADIUS_M=250.0, STOP_DWELL_SEC=1800, GAP_HARD_SEC=18000, GAP_VERY_HARD_SEC=36000, DIST_AFTER_GAP_M=150000.0, JUMP_HARD_M=250000.0, DEST_GAP_SEC=14400, DEST_DIST_M=50000.0\n",
      "tsorted_built_at: b'2026-01-12T10:46:51'\n",
      "version: 1.0\n"
     ]
    }
   ],
   "source": [
    "if \"ds_out\" in vars():\n",
    "    ds_out.close()   # type: ignore\n",
    "\n",
    "ds_out = h5py.File(OUT_PATH, \"r\")\n",
    "for attr in ds_out.attrs:\n",
    "    print(f\"{attr}: {ds_out.attrs[attr]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72c097d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HDF5 Dataset Structure:\n",
      "\n",
      "\n",
      "files:\n",
      "  - file_id: int32\n",
      "  - name: |S256\n",
      "  - positions_count: int32\n",
      "  - timestamp: int32\n",
      "  Example: (1, b'29.10.2024_11_35.json', 43344, 1752416768)\n",
      "\n",
      "\n",
      "positions:\n",
      "  /YYYY\n",
      "    /MM\n",
      "      /DD:\n",
      "        - ship_id: int64\n",
      "        - timestamp: int32\n",
      "        - lat: float32\n",
      "        - lon: float32\n",
      "        - speed: int32\n",
      "        - course: int32\n",
      "        - heading: int32\n",
      "        - rot: int32\n",
      "        - elapsed: int32\n",
      "        - destination: |S64\n",
      "        - tile_z: int32\n",
      "        - file_id: int32\n",
      "        - track_id: int64\n",
      "        Example: (1, 1730200428, 43.0834, 132.3136, 0, 18, 166, 0, 338, b'NOVIIMIR', 10, 0, 1)\n",
      "  /tracks\n",
      "    /AAAA-BBBB\n",
      "      /CCCC-DDDD:\n",
      "        - track_id: int64\n",
      "        - timestamp: int32\n",
      "        - lat: float32\n",
      "        - lon: float32\n",
      "        - speed: int32\n",
      "        - course: int32\n",
      "        - heading: int32\n",
      "        - rot: int32\n",
      "        - elapsed: int32\n",
      "        - ship_id: int64\n",
      "        - file_id: int32\n",
      "        - destination: |S64\n",
      "        - tile_z: int32\n",
      "        Example: (161, 1730200666, 39.23588, 122.5899, 239, 278, -1, 0, 103, 214, 0, b'CLASS B', 5)\n",
      "\n",
      "\n",
      "ships:\n",
      "  - ship_id: int64\n",
      "  - mt_id: |S128\n",
      "  - name: |S128\n",
      "  - flag: |S4\n",
      "  - ship_type: int32\n",
      "  - gt_ship_type: int32\n",
      "  - length: int32\n",
      "  - width: int32\n",
      "  - dwt: int32\n",
      "  Example: (3, b'756215', b'DONG LONG', b'PA', 7, 6, 189, 32, 52478)\n",
      "\n",
      "\n",
      "tracks:\n",
      "  - track_id: int64\n",
      "  - ship_id: int64\n",
      "  - start_timestamp: int32\n",
      "  - end_timestamp: int32\n",
      "  - start_lat: float32\n",
      "  - start_lon: float32\n",
      "  - end_lat: float32\n",
      "  - end_lon: float32\n",
      "  - points_count: int32\n",
      "  Example: (1811, 2756, 1730199770, 1730200334, 32.37317, 134.1212, 32.37317, 134.1212, 2)\n",
      "\n",
      "\n",
      "zones:\n",
      "  - zone_id: int64\n",
      "  - name: |S256\n",
      "  - lat: float32\n",
      "  - lon: float32\n",
      "  - zoom: int32\n",
      "  Example: (1, b'\\xd0\\x92\\xd0\\xb5\\xd1\\x81\\xd1\\x8c \\xd1\\x80\\xd0\\xb5\\xd0\\xb3\\xd0\\xb8\\xd0\\xbe\\xd0\\xbd 1', 47.7, 145., 5)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"HDF5 Dataset Structure:\\n\\n\")\n",
    "\n",
    "for name, obj in ds_out.items():\n",
    "    print(f\"{name}:\")\n",
    "    if isinstance(obj, h5py.Dataset) and obj.dtype.fields:\n",
    "        for field_name, (field_dtype, offset, *_) in obj.dtype.fields.items():\n",
    "            print(f\"  - {field_name}: {field_dtype}\")\n",
    "        if len(obj) >= 1:\n",
    "            print(f\"  Example: {obj[1]}\")\n",
    "    if isinstance(obj, h5py.Group):\n",
    "        if name == \"positions\" and \"tracks\" in obj:\n",
    "            print(f\"  /YYYY\\n    /MM\\n      /DD:\")\n",
    "            for k1, item in obj.items():\n",
    "                if k1 == \"tracks\":\n",
    "                    continue\n",
    "                for _, item in item.items():\n",
    "                    for _, item in item.items():\n",
    "                        for field_name, (field_dtype, offset, *_) in item.dtype.fields.items():\n",
    "                            print(f\"        - {field_name}: {field_dtype}\")\n",
    "                        print(f\"        Example: {item[0]}\")\n",
    "                        break\n",
    "                    break\n",
    "                break\n",
    "\n",
    "            print(f\"  /tracks\\n    /AAAA-BBBB\\n      /CCCC-DDDD:\")\n",
    "            for _, item in obj[\"tracks\"].items():\n",
    "                for _, item in item.items():\n",
    "                    for field_name, (field_dtype, offset, *_) in item.dtype.fields.items():\n",
    "                        print(f\"        - {field_name}: {field_dtype}\")\n",
    "                    print(f\"        Example: {item[0]}\")\n",
    "                    break\n",
    "                break\n",
    "        else:\n",
    "            print(f\"  /YYYY\\n    /MM\\n      /DD:\")\n",
    "            for _, item in obj.items():\n",
    "                for _, item in item.items():\n",
    "                    for _, item in item.items():\n",
    "                        for field_name, (field_dtype, offset, *_) in item.dtype.fields.items():\n",
    "                            print(f\"        - {field_name}: {field_dtype}\")\n",
    "                        print(f\"        Example: {item[0]}\")\n",
    "                        break\n",
    "                    break\n",
    "                break\n",
    "    print(\"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
